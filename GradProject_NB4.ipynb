{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Extension Activities</h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (Optional)- Now, try using TensorFlow to categorize your images. The accuracy should be significantly higher due to the usage of nueral nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheng/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "(1501, 256, 256, 3) (1501,)\n"
     ]
    }
   ],
   "source": [
    "#Dataset processing\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def load_data(train_path, img_w, img_h):\n",
    "    print('Reading training images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    for index, fld in enumerate(classes):\n",
    "        path = os.path.join(train_path, fld, '*g')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            image = cv2.imread(fl)\n",
    "            image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "            img_list.append(image), label_list.append(index)\n",
    "    img_list, label_list = np.array(img_list, dtype=np.uint8), np.array( label_list, dtype=np.int32 )\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, label_list\n",
    "\n",
    "img_w, img_h = 256, 256\n",
    "train_dir, test_dir = \"20_categories_training/\", \"20_Validation/\"\n",
    "\n",
    "data, label = load_data(train_dir, img_w, img_h)\n",
    "print(data.shape, label.shape)\n",
    "idx = np.arange( data.shape[0] )\n",
    "np.random.shuffle(idx)\n",
    "data, label = data[idx], label[idx]\n",
    "\n",
    "split_ratio = 0.9\n",
    "split = np.int(data.shape[0]* split_ratio)\n",
    "x_train, y_train = data[:split], label[:split]\n",
    "x_val, y_val =data[split:], label[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow model\n",
    "def inference(input_tensor, batch_size, n_classes, train):\n",
    "    # conv1\n",
    "    with tf.variable_scope(\"layer1-conv1\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 3, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(activation, name=\"conv1\")\n",
    "\n",
    "    # pool1 && norm1\n",
    "    with tf.variable_scope(\"layer2-pooling_lrn_1\") as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling1\")\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope(\"layer3-conv2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 16, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(activation, name=\"conv2\")\n",
    "\n",
    "    # pool2 && norm2\n",
    "    with tf.variable_scope(\"layer4-pooling2_lrn\") as scope:\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling2\")\n",
    "        norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm2')\n",
    "#         print(norm2.shape)\n",
    "    # full-connect1\n",
    "    with tf.variable_scope(\"layer5-fc1\") as scope:\n",
    "        reshape = tf.reshape(norm2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable(\"weights\", shape=[65536, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=\"fc1\")\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    # full_connect2\n",
    "    with tf.variable_scope(\"layer6-fc2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=\"fc2\")\n",
    "        if train: fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "\n",
    "    # softmax\n",
    "    with tf.variable_scope(\"layer7-fc3\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, n_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[n_classes], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.add(tf.matmul(fc2, weights), biases, name=\"fc3\")\n",
    "    return logit\n",
    "\n",
    "channel, num_classes = 3, 20\n",
    "model_path = \"model/model.ckpt\"\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_w, img_h, channel], name='x')\n",
    "labels = tf.placeholder(tf.int32, shape=[None,], name='label')\n",
    "\n",
    "# regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "logits = inference(x, batch_size=batch_size, n_classes=num_classes, train=True)\n",
    "\n",
    "b = tf.constant(value=1,dtype=tf.float32)\n",
    "logits_eval = tf.multiply(logits,b,name='logits_eval') \n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "gold = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), labels)    \n",
    "acc = tf.reduce_mean(tf.cast(gold, tf.float32))\n",
    "\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 190.646124\n",
      "   train acc: 0.078869\n",
      "   validation loss: 190.196976\n",
      "   validation acc: 0.085938\n",
      "   train loss: 186.449254\n",
      "   train acc: 0.114583\n",
      "   validation loss: 177.802826\n",
      "   validation acc: 0.156250\n",
      "   train loss: 175.131278\n",
      "   train acc: 0.170387\n",
      "   validation loss: 164.359985\n",
      "   validation acc: 0.179688\n",
      "   train loss: 168.182850\n",
      "   train acc: 0.204613\n",
      "   validation loss: 157.688431\n",
      "   validation acc: 0.234375\n",
      "   train loss: 160.565034\n",
      "   train acc: 0.248512\n",
      "   validation loss: 153.880447\n",
      "   validation acc: 0.265625\n",
      "   train loss: 154.476981\n",
      "   train acc: 0.265625\n",
      "   validation loss: 149.247223\n",
      "   validation acc: 0.289062\n",
      "   train loss: 149.821359\n",
      "   train acc: 0.281994\n",
      "   validation loss: 142.804626\n",
      "   validation acc: 0.289062\n",
      "   train loss: 146.849144\n",
      "   train acc: 0.290923\n",
      "   validation loss: 140.171204\n",
      "   validation acc: 0.320312\n",
      "   train loss: 143.176165\n",
      "   train acc: 0.309524\n",
      "   validation loss: 146.228104\n",
      "   validation acc: 0.273438\n",
      "   train loss: 138.795015\n",
      "   train acc: 0.318452\n",
      "   validation loss: 140.300919\n",
      "   validation acc: 0.312500\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, labels: y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch += 1\n",
    "    print(\"------------%d------------\" % epoch)\n",
    "    print(\"   train loss: %f\" % (np.sum(train_loss)/ n_batch))\n",
    "    print(\"   train acc: %f\" % ( np.sum(train_acc)/ n_batch))\n",
    "\n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, labels: y_val_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\"   validation loss: %f\" % (np.sum(val_loss)/ n_batch))\n",
    "    print(\"   validation acc: %f\" % (np.sum(val_acc)/ n_batch))\n",
    "saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File model/model.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c1f42009c30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/model.ckpt.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1950\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File model/model.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "def load_test(train_path, test_path, img_w, img_h):\n",
    "    print('Reading testing images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    files = glob.glob(test_path)\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "    img_list = np.array(img_list, dtype=np.uint8)\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, classes\n",
    "\n",
    "test_data = load_test(train_path, test_path, img_w, img_h)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model/model.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('model/'))\n",
    "    graph = tf.get_default_graph()\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    feed_dict = {x:test_data}\n",
    "    logits = graph.get_tensor_by_name(\"logits_eval:0\")\n",
    "    classification_result = sess.run(logits,feed_dict)\n",
    "#     print(tf.argmax(classification_result,1).eval())\n",
    "    output = tf.argmax(classification_result, 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF():\n",
    "#     raise NotImplemented()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
