{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Extension Activities</h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (Optional)- Now, try using TensorFlow to categorize your images. The accuracy should be significantly higher due to the usage of nueral nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "(1501, 256, 256, 3) (1501,)\n"
     ]
    }
   ],
   "source": [
    "#Dataset processing\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def load_data(train_path, img_w, img_h):\n",
    "    print('Reading training images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    for index, fld in enumerate(classes):\n",
    "        path = os.path.join(train_path, fld, '*g')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            image = cv2.imread(fl)\n",
    "            image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "            img_list.append(image), label_list.append(index)\n",
    "    img_list, label_list = np.array(img_list, dtype=np.uint8), np.array( label_list, dtype=np.int32 )\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, label_list\n",
    "\n",
    "img_w, img_h = 256, 256\n",
    "train_dir, test_dir = \"train/\", \"test/\"\n",
    "\n",
    "data, label = load_data(train_dir, img_w, img_h)\n",
    "print(data.shape, label.shape)\n",
    "idx = np.arange( data.shape[0] )\n",
    "np.random.shuffle(idx)\n",
    "data, label = data[idx], label[idx]\n",
    "\n",
    "split_ratio = 0.9\n",
    "split = np.int(data.shape[0]* split_ratio)\n",
    "x_train, y_train = data[:split], label[:split]\n",
    "x_val, y_val =data[split:], label[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow model\n",
    "def inference(input_tensor, batch_size, n_classes, train):\n",
    "    # conv1\n",
    "    with tf.variable_scope(\"layer1-conv1\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 3, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(activation, name=\"conv1\")\n",
    "\n",
    "    # pool1 && norm1\n",
    "    with tf.variable_scope(\"layer2-pooling_lrn_1\") as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling1\")\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope(\"layer3-conv2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 16, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(activation, name=\"conv2\")\n",
    "\n",
    "    # pool2 && norm2\n",
    "    with tf.variable_scope(\"layer4-pooling2_lrn\") as scope:\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling2\")\n",
    "        norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm2')\n",
    "#         print(norm2.shape)\n",
    "    # full-connect1\n",
    "    with tf.variable_scope(\"layer5-fc1\") as scope:\n",
    "        reshape = tf.reshape(norm2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable(\"weights\", shape=[65536, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=\"fc1\")\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    # full_connect2\n",
    "    with tf.variable_scope(\"layer6-fc2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=\"fc2\")\n",
    "        if train: fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "\n",
    "    # softmax\n",
    "    with tf.variable_scope(\"layer7-fc3\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, n_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[n_classes], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.add(tf.matmul(fc2, weights), biases, name=\"fc3\")\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "    return logit\n",
    "\n",
    "channel, num_classes = 3, 20\n",
    "model_path = \"model/model.ckpt\"\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_w, img_h, channel], name='x')\n",
    "labels = tf.placeholder(tf.int32, shape=[None,], name='label')\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "logits = inference(x, batch_size=batch_size, n_classes=num_classes, train=True)\n",
    "\n",
    "b = tf.constant(value=1,dtype=tf.float32)\n",
    "logits_eval = tf.multiply(logits,b,name='logits_eval') \n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "gold = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), labels)    \n",
    "acc = tf.reduce_mean(tf.cast(gold, tf.float32))\n",
    "\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0------------\n",
      "   train loss: 191.064523\n",
      "   train acc: 0.086310\n",
      "   validation loss: 187.787628\n",
      "   validation acc: 0.117188\n",
      "------------Epoch: 1------------\n",
      "   train loss: 188.466367\n",
      "   train acc: 0.109375\n",
      "   validation loss: 187.642487\n",
      "   validation acc: 0.179688\n",
      "------------Epoch: 2------------\n",
      "   train loss: 184.459426\n",
      "   train acc: 0.131696\n",
      "   validation loss: 176.697052\n",
      "   validation acc: 0.179688\n",
      "------------Epoch: 3------------\n",
      "   train loss: 174.035970\n",
      "   train acc: 0.182292\n",
      "   validation loss: 164.664810\n",
      "   validation acc: 0.265625\n",
      "------------Epoch: 4------------\n",
      "   train loss: 163.321708\n",
      "   train acc: 0.229167\n",
      "   validation loss: 158.522827\n",
      "   validation acc: 0.296875\n",
      "------------Epoch: 5------------\n",
      "   train loss: 155.182361\n",
      "   train acc: 0.270833\n",
      "   validation loss: 154.477539\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 6------------\n",
      "   train loss: 151.355085\n",
      "   train acc: 0.264881\n",
      "   validation loss: 150.044022\n",
      "   validation acc: 0.281250\n",
      "------------Epoch: 7------------\n",
      "   train loss: 146.677141\n",
      "   train acc: 0.276786\n",
      "   validation loss: 145.068085\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 8------------\n",
      "   train loss: 146.037621\n",
      "   train acc: 0.293899\n",
      "   validation loss: 145.014954\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 9------------\n",
      "   train loss: 142.132557\n",
      "   train acc: 0.290923\n",
      "   validation loss: 137.345032\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 10------------\n",
      "   train loss: 137.255406\n",
      "   train acc: 0.317708\n",
      "   validation loss: 141.217880\n",
      "   validation acc: 0.343750\n",
      "------------Epoch: 11------------\n",
      "   train loss: 134.939395\n",
      "   train acc: 0.309524\n",
      "   validation loss: 134.255966\n",
      "   validation acc: 0.382812\n",
      "------------Epoch: 12------------\n",
      "   train loss: 133.890718\n",
      "   train acc: 0.352679\n",
      "   validation loss: 146.951859\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 13------------\n",
      "   train loss: 132.067557\n",
      "   train acc: 0.349702\n",
      "   validation loss: 137.880341\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 14------------\n",
      "   train loss: 127.080566\n",
      "   train acc: 0.350446\n",
      "   validation loss: 142.916946\n",
      "   validation acc: 0.312500\n",
      "------------Epoch: 15------------\n",
      "   train loss: 126.009905\n",
      "   train acc: 0.365327\n",
      "   validation loss: 146.416885\n",
      "   validation acc: 0.335938\n",
      "------------Epoch: 16------------\n",
      "   train loss: 122.322045\n",
      "   train acc: 0.371280\n",
      "   validation loss: 135.561203\n",
      "   validation acc: 0.375000\n",
      "------------Epoch: 17------------\n",
      "   train loss: 120.588332\n",
      "   train acc: 0.376488\n",
      "   validation loss: 137.073837\n",
      "   validation acc: 0.351562\n",
      "------------Epoch: 18------------\n",
      "   train loss: 118.759742\n",
      "   train acc: 0.374256\n",
      "   validation loss: 151.865997\n",
      "   validation acc: 0.343750\n",
      "------------Epoch: 19------------\n",
      "   train loss: 112.635510\n",
      "   train acc: 0.406250\n",
      "   validation loss: 147.982178\n",
      "   validation acc: 0.296875\n",
      "------------Epoch: 20------------\n",
      "   train loss: 109.959938\n",
      "   train acc: 0.427083\n",
      "   validation loss: 140.671997\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 21------------\n",
      "   train loss: 107.247280\n",
      "   train acc: 0.434524\n",
      "   validation loss: 148.635376\n",
      "   validation acc: 0.375000\n",
      "------------Epoch: 22------------\n",
      "   train loss: 108.367757\n",
      "   train acc: 0.424851\n",
      "   validation loss: 145.607910\n",
      "   validation acc: 0.328125\n",
      "------------Epoch: 23------------\n",
      "   train loss: 102.981562\n",
      "   train acc: 0.464286\n",
      "   validation loss: 143.975433\n",
      "   validation acc: 0.382812\n",
      "------------Epoch: 24------------\n",
      "   train loss: 99.660133\n",
      "   train acc: 0.465030\n",
      "   validation loss: 142.350662\n",
      "   validation acc: 0.351562\n",
      "------------Epoch: 25------------\n",
      "   train loss: 101.348586\n",
      "   train acc: 0.449405\n",
      "   validation loss: 142.370728\n",
      "   validation acc: 0.351562\n",
      "------------Epoch: 26------------\n",
      "   train loss: 95.742001\n",
      "   train acc: 0.487351\n",
      "   validation loss: 152.977921\n",
      "   validation acc: 0.398438\n",
      "------------Epoch: 27------------\n",
      "   train loss: 94.982445\n",
      "   train acc: 0.477679\n",
      "   validation loss: 138.518066\n",
      "   validation acc: 0.390625\n",
      "------------Epoch: 28------------\n",
      "   train loss: 90.259184\n",
      "   train acc: 0.513393\n",
      "   validation loss: 151.701675\n",
      "   validation acc: 0.390625\n",
      "------------Epoch: 29------------\n",
      "   train loss: 86.827050\n",
      "   train acc: 0.529018\n",
      "   validation loss: 148.920471\n",
      "   validation acc: 0.367188\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 30       \n",
    "best_val_acc = 0.0\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, labels: y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch += 1\n",
    "    print(\"------------Epoch: %d------------\" % epoch)\n",
    "    print(\"   train loss: %f\" % (np.sum(train_loss)/ n_batch))\n",
    "    print(\"   train acc: %f\" % ( np.sum(train_acc)/ n_batch))\n",
    "\n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, labels: y_val_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\"   validation loss: %f\" % (np.sum(val_loss)/ n_batch))\n",
    "    print(\"   validation acc: %f\" % (np.sum(val_acc)/ n_batch))\n",
    "    if np.sum(val_acc)/n_batch > best_val_acc:\n",
    "        best_val_acc = np.sum(val_acc)/n_batch\n",
    "        saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing images\n",
      "(716, 256, 256, 3)\n",
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "716\n"
     ]
    }
   ],
   "source": [
    "def load_test(train_path, test_path, img_w, img_h):\n",
    "    print('Reading testing images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    files = glob.glob(test_path + '*g')\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "        img_list.append( image )\n",
    "    img_list = np.array(img_list, dtype=np.uint8)\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, classes, files\n",
    "\n",
    "test_data, _, test_files = load_test(train_dir, test_dir, img_w, img_h)\n",
    "print(test_data.shape)\n",
    "total_outputs = []\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model/model.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('model/'))\n",
    "    graph = tf.get_default_graph()\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    for idx in range(0, test_data.shape[0], batch_size):\n",
    "        if idx + batch_size > test_data.shape[0]:\n",
    "            cur_pack = test_data[-batch_size:]\n",
    "            cur_left = test_data.shape[0] % batch_size\n",
    "        else:\n",
    "            cur_pack = test_data[idx:idx+batch_size]\n",
    "            cur_left = batch_size\n",
    "        feed_dict = {x:cur_pack}\n",
    "        logits = graph.get_tensor_by_name(\"logits_eval:0\")\n",
    "        classification_result = sess.run(logits,feed_dict)\n",
    "        output = tf.argmax(classification_result, 1).eval()\n",
    "        total_outputs.extend( output.tolist()[-cur_left:] )    \n",
    "print(len(total_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25970"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "res = {x:y for x, y in zip(test_files, total_outputs)}\n",
    "open(\"test.json\", \"w\").write( json.dumps( res ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def TF():\n",
    "#     raise NotImplemented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
