{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Extension Activities</h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (Optional)- Now, try using TensorFlow to categorize your images. The accuracy should be significantly higher due to the usage of nueral nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "(1501, 224, 224, 3) (1501,)\n"
     ]
    }
   ],
   "source": [
    "#Dataset processing\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def short_side_resize(image, length=256):\n",
    "    height, width, _ = image.shape\n",
    "    if height < width:\n",
    "        ratio = length / height\n",
    "    else:\n",
    "        ratio = length / width\n",
    "    new_height = int(height * ratio)\n",
    "    new_width = int(width * ratio)\n",
    "    return cv2.resize(image, (new_width, new_height), cv2.INTER_LINEAR)\n",
    "\n",
    "def center_crop(image, length=224):\n",
    "    height, width, _ = image.shape\n",
    "    cx, cy = height // 2, width // 2\n",
    "    lx, ly = cx - length//2, cy - length//2\n",
    "    hx, hy = length + lx, length + ly\n",
    "    return image[lx:hx, ly:hy, :]\n",
    "\n",
    "def load_data(train_path, img_w, img_h):\n",
    "    print('Reading training images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    for index, fld in enumerate(classes):\n",
    "        path = os.path.join(train_path, fld, '*g')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            image = cv2.imread(fl)\n",
    "#             image = center_crop(short_side_resize(image))\n",
    "            image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "            img_list.append(image), label_list.append(index)\n",
    "    img_list, label_list = np.array(img_list, dtype=np.uint8), np.array( label_list, dtype=np.int32 )\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, label_list\n",
    "\n",
    "img_w, img_h = 224, 224\n",
    "train_dir, test_dir = \"train/\", \"test/\"\n",
    "\n",
    "data, label = load_data(train_dir, img_w, img_h)\n",
    "print(data.shape, label.shape)\n",
    "idx = np.arange( data.shape[0] )\n",
    "np.random.shuffle(idx)\n",
    "data, label = data[idx], label[idx]\n",
    "\n",
    "split_ratio = 0.9\n",
    "split = np.int(data.shape[0]* split_ratio)\n",
    "x_train, y_train = data[:split], label[:split]\n",
    "x_val, y_val =data[split:], label[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 56, 56, 16)\n"
     ]
    }
   ],
   "source": [
    "#tensorflow model\n",
    "def inference(input_tensor, batch_size, n_classes, train):\n",
    "    # conv1\n",
    "    with tf.variable_scope(\"layer1-conv1\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 3, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(activation, name=\"conv1\")\n",
    "\n",
    "    # pool1 && norm1\n",
    "    with tf.variable_scope(\"layer2-pooling_lrn_1\") as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling1\")\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope(\"layer3-conv2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[3, 3, 16, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(activation, name=\"conv2\")\n",
    "\n",
    "    # pool2 && norm2\n",
    "    with tf.variable_scope(\"layer4-pooling2_lrn\") as scope:\n",
    "        pool2 = tf.nn.avg_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling2\")\n",
    "#         pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pooling2\")\n",
    "        norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='layer_norm2')\n",
    "        print(norm2.shape)\n",
    "    # full-connect1\n",
    "    with tf.variable_scope(\"layer5-fc1\") as scope:\n",
    "#         reshape = tf.reduce_mean( norm2, axis=[-1] )\n",
    "        reshape = tf.reshape(norm2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable(\"weights\", shape=[50176, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=\"fc1\")\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    # full_connect2\n",
    "    with tf.variable_scope(\"layer6-fc2\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=\"fc2\")\n",
    "        if train: fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "\n",
    "    # softmax\n",
    "    with tf.variable_scope(\"layer7-fc3\") as scope:\n",
    "        weights = tf.get_variable(\"weights\", shape=[128, n_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable(\"biases\", shape=[n_classes], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.add(tf.matmul(fc2, weights), biases, name=\"fc3\")\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(biases))\n",
    "    return logit\n",
    "\n",
    "channel, num_classes = 3, 20\n",
    "model_path = \"model/model.ckpt\"\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_w, img_h, channel], name='x')\n",
    "labels = tf.placeholder(tf.int32, shape=[None,], name='label')\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "logits = inference(x, batch_size=batch_size, n_classes=num_classes, train=True)\n",
    "\n",
    "b = tf.constant(value=1,dtype=tf.float32)\n",
    "logits_eval = tf.multiply(logits,b,name='logits_eval') \n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "gold = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), labels)    \n",
    "acc = tf.reduce_mean(tf.cast(gold, tf.float32))\n",
    "\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0------------\n",
      "   train loss: 191.042178\n",
      "   train acc: 0.080357\n",
      "   validation loss: 186.924591\n",
      "   validation acc: 0.132812\n",
      "------------Epoch: 1------------\n",
      "   train loss: 188.938976\n",
      "   train acc: 0.097470\n",
      "   validation loss: 184.703186\n",
      "   validation acc: 0.179688\n",
      "------------Epoch: 2------------\n",
      "   train loss: 180.395775\n",
      "   train acc: 0.136905\n",
      "   validation loss: 171.489182\n",
      "   validation acc: 0.203125\n",
      "------------Epoch: 3------------\n",
      "   train loss: 171.000023\n",
      "   train acc: 0.180804\n",
      "   validation loss: 162.266632\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 4------------\n",
      "   train loss: 160.991874\n",
      "   train acc: 0.247024\n",
      "   validation loss: 153.049286\n",
      "   validation acc: 0.312500\n",
      "------------Epoch: 5------------\n",
      "   train loss: 152.329334\n",
      "   train acc: 0.258185\n",
      "   validation loss: 149.890060\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 6------------\n",
      "   train loss: 148.496745\n",
      "   train acc: 0.270833\n",
      "   validation loss: 152.686829\n",
      "   validation acc: 0.312500\n",
      "------------Epoch: 7------------\n",
      "   train loss: 146.121198\n",
      "   train acc: 0.299851\n",
      "   validation loss: 156.515121\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 8------------\n",
      "   train loss: 144.178909\n",
      "   train acc: 0.300595\n",
      "   validation loss: 142.808899\n",
      "   validation acc: 0.359375\n",
      "------------Epoch: 9------------\n",
      "   train loss: 138.586275\n",
      "   train acc: 0.303571\n",
      "   validation loss: 155.223755\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 10------------\n",
      "   train loss: 138.398844\n",
      "   train acc: 0.312500\n",
      "   validation loss: 155.722519\n",
      "   validation acc: 0.289062\n",
      "------------Epoch: 11------------\n",
      "   train loss: 137.637440\n",
      "   train acc: 0.319196\n",
      "   validation loss: 148.391235\n",
      "   validation acc: 0.289062\n",
      "------------Epoch: 12------------\n",
      "   train loss: 134.738037\n",
      "   train acc: 0.323661\n",
      "   validation loss: 151.181595\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 13------------\n",
      "   train loss: 133.348307\n",
      "   train acc: 0.331101\n",
      "   validation loss: 150.748734\n",
      "   validation acc: 0.359375\n",
      "------------Epoch: 14------------\n",
      "   train loss: 131.597912\n",
      "   train acc: 0.344494\n",
      "   validation loss: 152.008408\n",
      "   validation acc: 0.328125\n",
      "------------Epoch: 15------------\n",
      "   train loss: 131.544596\n",
      "   train acc: 0.330357\n",
      "   validation loss: 153.965912\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 16------------\n",
      "   train loss: 128.477865\n",
      "   train acc: 0.360119\n",
      "   validation loss: 151.185867\n",
      "   validation acc: 0.289062\n",
      "------------Epoch: 17------------\n",
      "   train loss: 124.077927\n",
      "   train acc: 0.369048\n",
      "   validation loss: 159.684738\n",
      "   validation acc: 0.312500\n",
      "------------Epoch: 18------------\n",
      "   train loss: 125.913830\n",
      "   train acc: 0.338542\n",
      "   validation loss: 151.382401\n",
      "   validation acc: 0.320312\n",
      "------------Epoch: 19------------\n",
      "   train loss: 123.522368\n",
      "   train acc: 0.369792\n",
      "   validation loss: 162.101440\n",
      "   validation acc: 0.351562\n",
      "------------Epoch: 20------------\n",
      "   train loss: 121.717262\n",
      "   train acc: 0.375744\n",
      "   validation loss: 166.667221\n",
      "   validation acc: 0.328125\n",
      "------------Epoch: 21------------\n",
      "   train loss: 121.433780\n",
      "   train acc: 0.362351\n",
      "   validation loss: 156.832275\n",
      "   validation acc: 0.367188\n",
      "------------Epoch: 22------------\n",
      "   train loss: 120.029181\n",
      "   train acc: 0.366071\n",
      "   validation loss: 155.391388\n",
      "   validation acc: 0.367188\n",
      "------------Epoch: 23------------\n",
      "   train loss: 118.445685\n",
      "   train acc: 0.377232\n",
      "   validation loss: 164.587311\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 24------------\n",
      "   train loss: 115.854632\n",
      "   train acc: 0.389137\n",
      "   validation loss: 147.092834\n",
      "   validation acc: 0.328125\n",
      "------------Epoch: 25------------\n",
      "   train loss: 111.900298\n",
      "   train acc: 0.415179\n",
      "   validation loss: 169.832886\n",
      "   validation acc: 0.304688\n",
      "------------Epoch: 26------------\n",
      "   train loss: 111.079520\n",
      "   train acc: 0.414435\n",
      "   validation loss: 155.721039\n",
      "   validation acc: 0.390625\n",
      "------------Epoch: 27------------\n",
      "   train loss: 108.893206\n",
      "   train acc: 0.434524\n",
      "   validation loss: 161.820557\n",
      "   validation acc: 0.328125\n",
      "------------Epoch: 28------------\n",
      "   train loss: 108.892346\n",
      "   train acc: 0.418155\n",
      "   validation loss: 168.771652\n",
      "   validation acc: 0.296875\n",
      "------------Epoch: 29------------\n",
      "   train loss: 109.132382\n",
      "   train acc: 0.407738\n",
      "   validation loss: 181.875702\n",
      "   validation acc: 0.351562\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 30       \n",
    "best_val_acc = 0.0\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, labels: y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch += 1\n",
    "    print(\"------------Epoch: %d------------\" % epoch)\n",
    "    print(\"   train loss: %f\" % (np.sum(train_loss)/ n_batch))\n",
    "    print(\"   train acc: %f\" % ( np.sum(train_acc)/ n_batch))\n",
    "\n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, labels: y_val_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\"   validation loss: %f\" % (np.sum(val_loss)/ n_batch))\n",
    "    print(\"   validation acc: %f\" % (np.sum(val_acc)/ n_batch))\n",
    "    if np.sum(val_acc)/n_batch > best_val_acc:\n",
    "        best_val_acc = np.sum(val_acc)/n_batch\n",
    "        saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing images\n",
      "(716, 224, 224, 3)\n",
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "716\n"
     ]
    }
   ],
   "source": [
    "def load_test(train_path, test_path, img_w, img_h):\n",
    "    print('Reading testing images')\n",
    "    img_list, label_list = [], []\n",
    "    classes = os.listdir(train_path)\n",
    "    files = glob.glob(test_path + '*g')\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        #image = center_crop(short_side_resize(image))\n",
    "        image = cv2.resize(image, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "        img_list.append( image )\n",
    "    img_list = np.array(img_list, dtype=np.uint8)\n",
    "    img_list = img_list.astype('float32')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, classes, files\n",
    "\n",
    "test_data, _, test_files = load_test(train_dir, test_dir, img_w, img_h)\n",
    "print(test_data.shape)\n",
    "total_outputs = []\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model/model.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('model/'))\n",
    "    graph = tf.get_default_graph()\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    for idx in range(0, test_data.shape[0], batch_size):\n",
    "        if idx + batch_size > test_data.shape[0]:\n",
    "            cur_pack = test_data[-batch_size:]\n",
    "            cur_left = test_data.shape[0] % batch_size\n",
    "        else:\n",
    "            cur_pack = test_data[idx:idx+batch_size]\n",
    "            cur_left = batch_size\n",
    "        feed_dict = {x:cur_pack}\n",
    "        logits = graph.get_tensor_by_name(\"logits_eval:0\")\n",
    "        classification_result = sess.run(logits,feed_dict)\n",
    "        output = tf.argmax(classification_result, 1).eval()\n",
    "        total_outputs.extend( output.tolist()[-cur_left:] )    \n",
    "print(len(total_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26011"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "res = {x:y for x, y in zip(test_files, total_outputs)}\n",
    "open(\"test.json\", \"w\").write( json.dumps( res ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def TF():\n",
    "#     raise NotImplemented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
